---
layout: default
title: Push Yolov8n Pre-train Model To rk3588
categories:
- Linux
tags:
- Server
- Linux
---
//Description: è®°å½•æŠŠYolov8né¢„è®­ç»ƒçš„æ¨¡å‹pushåˆ°rk3588çš„æ¿å­ä¸Šè¿›è¡Œç›®æ ‡æ£€æµ‹çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œå…¶ä¸­é‡åˆ°åŸå§‹æ¨¡å‹æ— æ³•jit.loadã€æ¨¡å‹è½¬æ¢åæ£€æµ‹é”™è¯¯ã€æ¿å­ä¸Šå®‰è£…SDKå¤±è´¥ç­‰é—®é¢˜ï¼Œæœ€ç»ˆé€šè¿‡æ”¹ç”¨loadã€åˆ†æerror listæŠ›å¼ƒéƒ¨åˆ†æ¨¡å‹ã€æ£€æŸ¥ç‰ˆæœ¬ä¸€è‡´æ€§è§£å†³ã€‚å†™äº2024å¹´4æœˆåˆã€‚

//Create Date: 2024-04-02 10:14:28

//Author: channy

[toc]

# åŸºæœ¬ç¯å¢ƒä¿¡æ¯
æ¿å­ä¸Šéœ€è¦ç‰ˆæœ¬ä¸€è‡´ Version (RKNN Server = Runtime = RKNN-Toolkit2)ã€‚aarch64ã€‚
* RKNN Server: 1.5.0
* RKNN Runtime (librknnrt): 1.5.0
* RKNPU driver: 0.9.2

PC ä½¿ç”¨äº†Ubuntuï¼Œå¿…éœ€è¦x86_64ï¼Œå› ä¸ºå®˜æ–¹çš„SDKä¸­åªæœ‰x64çš„whlå®‰è£…æ–‡ä»¶ï¼Œaarch64çš„åªèƒ½å®‰è£…ToolkitLite

# step1: ptæ¨¡å‹è½¬æ¢æˆonnxæ¨¡å‹
åŸæœ¬æ˜¯æƒ³è¦ç›´æ¥ä½¿ç”¨SDKä¸­çš„`rknn.load_pytorch(model = '../model/v8n.pt', input_size_list = [[1, 3, 1920, 1920]])`ç›´æ¥è½¬æ¢æˆrknnçš„ï¼Œä½†å‡ºç°äº†loadé”™è¯¯
## Q: torch.jit.load failed

```sh
RuntimeError('PytorchStreamReader failed locating file constants.pkl: file not found')
```

```
D Save log info to: build.log
I rknn-toolkit2 version: 2.0.0b0+9bab5682
W config: The quant_img_RGB2BGR of input 0 is set to True, which means that the RGB2BGR conversion will be done first
          when the quantized image is loaded (only valid for jpg/jpeg/png/bmp, npy will ignore this flag).
          Special note here, if quant_img_RGB2BGR is True and the quantized image is jpg/jpeg/png/bmp,
          the mean_values / std_values in the config corresponds the order of BGR.
W load_pytorch: Catch exception when torch.jit.load:
    RuntimeError('PytorchStreamReader failed locating file constants.pkl: file not found')
W load_pytorch: Make sure that the torch version of '/home/channy/Documents/thirdlibs/rknn_model_zoo/examples/yolov8/model/v8n.pt' is consistent with the installed torch version '2.2.1+cu121'!
E load_pytorch: Traceback (most recent call last):
E load_pytorch:   File "rknn/api/rknn_base.py", line 1590, in rknn.api.rknn_base.RKNNBase.load_pytorch
E load_pytorch:   File "/home/channy/miniconda3/envs/NewVersion/lib/python3.8/site-packages/torch/jit/_serialization.py", line 159, in load
E load_pytorch:     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]
E load_pytorch: RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found
W If you can't handle this error, please try updating to the latest version of the toolkit2 and runtime from:
  https://console.zbox.filez.com/l/I00fc3 (Pwd: rknn)  Path: RKNPU2_SDK / 2.X.X / develop /
  If the error still exists in the latest version, please collect the corresponding error logs and the model,
  convert script, and input data that can reproduce the problem, and then submit an issue on:
  https://redmine.rock-chips.com (Please consult our sales or FAE for the redmine account)
E build: The model has not been loaded, please load it first!
E export_rknn: RKNN model does not exist, please load & build model first!
E init_runtime: RKNN model does not exist, please load & build model first!
E inference: The runtime has not been initialized, please call init_runtime first!
```
ä¸Šç½‘æœç´¢å‘ç°ç¼ºå°‘`constants.pkl`å¤§æ¦‚ç‡æ˜¯å¯¼å‡ºæ¨¡å‹çš„æ—¶å€™è®¾ç½®çš„åªå¯¼å‡ºæƒé‡è¿˜æ˜¯å¯¼å‡ºæ•´ä¸ªç½‘ç»œå¯¼è‡´çš„ï¼Œä¸æ–¹ä¾¿é‡æ–°è®­ç»ƒå¯¼å‡ºï¼Œæ•…ç›´æ¥æ”¹ç”¨Yolov8è‡ªå¸¦çš„æ ¼å¼è½¬æ¢å‡½æ•°å…ˆè¡Œè½¬æ¢åˆ°`onnx`æ¨¡å‹ï¼Œè§é™„å½•ï¼‘ã€‚

# step 2: PCä¸ŠæŸ¥çœ‹onnxæ¨¡å‹çš„æ£€æµ‹ç»“æœ
è§é™„å½•2ã€‚æ£€æµ‹å›¾åƒä¸­æœ‰å¤šä¸ªç›®æ ‡ï¼Œèƒ½å¤Ÿæ­£å¸¸æ£€æµ‹åˆ°å…¶ä¸­ä¸¤ä¸ªç›®æ ‡ã€‚

# step 3: åœ¨PCä¸Šè½¬æ¢æˆrknnæ¨¡å‹å¹¶æ¨¡æ‹Ÿæ£€æµ‹
å…¶ä¸­`ret = rknn.load_onnx(model = model_path, outputs = export_outputs)`å‚æ•°outputsé»˜è®¤ä¸º`onnx`çš„æœ€ç»ˆè¾“å‡ºæ¨¡å‹ã€‚
## Q: ERROR 'REGTASK: The bit width of field value exceeds the limit'
```
E RKNN: [11:55:46.914] REGTASK: The bit width of field value exceeds the limit, target: v2, offset: 0x4038, shift = 16, limit: 0x1fff, value: 0x11f70
E RKNN: [11:55:46.914] REGTASK: The bit width of field value exceeds the limit, target: v2, offset: 0x5048, shift = 19, limit: 0x1fff, value: 0x11f70
```
æ¨¡å‹è½¬æ¢æŠ¥é”™`REGTASK: The bit width of field value exceeds the limit`ï¼Œä½†ä¾ç„¶ä¼šè¾“å‡ºè½¬æ¢åçš„rknnæ–‡ä»¶ã€‚

ä¸Šç½‘æœç´¢æœ‰è¯´åŠ `simplify=False`å‚æ•°çš„ï¼Œæœ‰è®©å¢åŠ æ··åˆé‡åŒ–çš„ï¼Œå°è¯•åå‡æ²¡æœ‰è§£å†³æŠ¥é”™é—®é¢˜ï¼Œæ¨¡å‹è½¬æ¢è¿‡ç¨‹ä¸­ä¾æ—§ä¼šæŠ¥ä¸Šé¢çš„é”™è¯¯ã€‚åˆæœç´¢åˆ°è¯´è¯¥æŠ¥é”™ä¸å½±å“çš„ï¼Œæš‚æ—¶
## Q: detect failed
æœ€åˆ`output`å‚æ•°æ²¡æœ‰è®¾ç½®ï¼Œç›´æ¥ä½¿ç”¨çš„é»˜è®¤å€¼ã€‚ç»“æœå‘ç°æ£€æµ‹å¤±è´¥ï¼Œæ²¡æœ‰ä»»ä½•æ£€æµ‹ç»“æœã€‚

ç„¶åä½¿ç”¨`ret = rknn.accuracy_analysis(inputs = ['../model/test.jpg'], target = None)`åˆ†æè¯¯å·®ã€‚åœ¨`snapshot`æ–‡ä»¶å¤¹ä¸‹`error_analysis.txt`æ–‡ä»¶ä¸­æŸ¥çœ‹æ¯ä¸€å±‚ç½‘ç»œPCæ¨¡æ‹Ÿå’ŒåŸæ¨¡å‹çš„è¯¯å·®å·®è·ã€‚

å‘ç°å®˜æ–¹æ¨¡å‹æ¯å±‚çš„æ¬§æ‹‰è·ç¦»éƒ½åœ¨6ä»¥å†…ï¼Œä½†è‡ªå·±çš„æ¨¡å‹æœ€åä¸¤å±‚æ¬§æ‹‰è·ç¦»è¾¾åˆ°180+ã€‚æ€€ç–‘æ˜¯æœ€åä¸¤å±‚æœ‰ä¸æ”¯æŒçš„ç®—å­å¯¼è‡´çš„è¯¯å·®å¤ªå¤§ä»è€Œæ£€æµ‹å¤±è´¥ã€‚
```
[Mul] /model.22/Mul_2_output_0_shape4                                               1.00000 | 181.08    1.00000 | 83.088        
[Reshape] /model.22/Mul_2_output_0                                                  1.00000 | 181.08    1.00000 | 83.088 
```

ä½¿ç”¨netronæŸ¥çœ‹åŸ`onnx`æ–‡ä»¶çš„ç½‘ç»œç»“æ„

![onnx_net](./../../images/onnx_net.png)

åœ¨`rknn.load_onnx(...)`ä¸­è®¾ç½®`outputs`å‚æ•°èˆå¼ƒè¯¯å·®å¤ªå¤§çš„å±‚ã€‚

å†é‡æ–°è½¬æ¢æˆrknnåæ¨¡æ‹Ÿæ£€æµ‹å‘ç°å¯ä»¥æ£€æµ‹åˆ°ç›®æ ‡äº†ã€‚

## Q: different results in simulator and PC
ç„¶åå‘ç°å¯¹åŒä¸€å¼ åŒ…å«å¤šä¸ªç›®æ ‡çš„å›¾åƒï¼ŒPCä¸Šä½¿ç”¨`onnx`æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹åˆ°2ä¸ªç›®æ ‡ï¼ŒPCæ¨¡æ‹Ÿä½¿ç”¨`rknn`ä¹Ÿèƒ½å¤Ÿæ£€æµ‹åˆ°2ä¸ªç›®æ ‡ï¼Œä½†ä¸æ˜¯ç›¸åŒçš„2ä¸ªç›®æ ‡ï¼Œå³æ£€æµ‹ç»“æœä¸å®Œå…¨ä¸€è‡´ï¼Œå‰è€…æ£€æµ‹åˆ°ç›®æ ‡Aå’ŒBï¼Œè€Œåè€…æ£€æµ‹åˆ°ç›®æ ‡Aå’ŒCã€‚

# step 4: åœ¨æ¿å­ä¸Šè¿è¡Œç›®æ ‡æ£€æµ‹
æŠŠç”Ÿæˆçš„rknnæ”¾åˆ°æ¿å­ä¸Šåï¼Œæ ¹æ®SDKæ–‡æ¡£åœ¨æ¿å­ä¸Šå®‰è£…SDKï¼Œå³ç›®å½•`rknn-toolkit-lite2/packages`ä¸‹çš„whlæ–‡ä»¶ã€‚ç¡®è®¤ç‰ˆæœ¬ä¸€è‡´ Version (RKNN Server = Runtime = RKNN-Toolkit2)ã€‚

å®‰è£…å®Œæˆåï¼Œåœ¨æ¿å­ä¸Šæ˜¯rknnliteï¼Œç›´æ¥åŠ è½½æ¨¡å‹ã€åˆå§‹åŒ–ï¼Œå³å¯ä»¥æ¨ç†ã€‚

```python
from rknnlite.api import RKNNLite

rknn_lite = RKNNLite()
ret = rknn_lite.load_rknn('../model/v8n.rknn')
ret = rknn_lite.init_runtime(core_mask=RKNNLite.NPU_CORE_0)

# read image same as in PC

outputs = rknn_lite.inference(inputs=[img])
# do something same as in PC
rknn_lite.release()
```
è¯¦ç»†å¯è§é™„å½•3ã€‚

***
<font color=red>é™„å½•</font>
***

# é™„å½•1: ptåˆ°onnxçš„æ¨¡å‹è½¬æ¢
```python
from ultralytics import YOLO

def pt2onnx(path = '../model/v8n.pt'):
    model = YOLO(path)
    res = model.export(format="onnx", opset = 19, simplify = True)  # export the model to ONNX format
```
# é™„å½•2: PCä¸ŠæŸ¥çœ‹onnxæ¨¡å‹çš„æ£€æµ‹ç»“æœ
```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import argparse

import cv2
import numpy as np
import onnxruntime as ort
import torch

from ultralytics.utils import ASSETS, yaml_load
from ultralytics.utils.checks import check_requirements, check_yaml


class YOLOv8:
    """YOLOv8 object detection model class for handling inference and visualization."""

    def __init__(self, onnx_model, input_image, confidence_thres, iou_thres):
        """
        Initializes an instance of the YOLOv8 class.

        Args:
            onnx_model: Path to the ONNX model.
            input_image: Path to the input image.
            confidence_thres: Confidence threshold for filtering detections.
            iou_thres: IoU (Intersection over Union) threshold for non-maximum suppression.
        """
        self.onnx_model = onnx_model
        self.input_image = input_image
        self.confidence_thres = confidence_thres
        self.iou_thres = iou_thres

        # Load the class names from the COCO dataset
        self.classes = yaml_load(check_yaml("coco128.yaml"))["names"]

        # Generate a color palette for the classes
        self.color_palette = np.random.uniform(0, 255, size=(len(self.classes), 3))

    def draw_detections(self, img, box, score, class_id):
        """
        Draws bounding boxes and labels on the input image based on the detected objects.

        Args:
            img: The input image to draw detections on.
            box: Detected bounding box.
            score: Corresponding detection score.
            class_id: Class ID for the detected object.

        Returns:
            None
        """

        # Extract the coordinates of the bounding box
        x1, y1, w, h = box

        # Retrieve the color for the class ID
        color = self.color_palette[class_id]

        # Draw the bounding box on the image
        cv2.rectangle(img, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)

        # Create the label text with class name and score
        label = f"{self.classes[class_id]}: {score:.2f}"

        # Calculate the dimensions of the label text
        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)

        # Calculate the position of the label text
        label_x = x1
        label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10

        # Draw a filled rectangle as the background for the label text
        cv2.rectangle(
            img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED
        )

        # Draw the label text on the image
        cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

    def preprocess(self):
        """
        Preprocesses the input image before performing inference.

        Returns:
            image_data: Preprocessed image data ready for inference.
        """
        # Read the input image using OpenCV
        self.img = cv2.imread(self.input_image)

        # Get the height and width of the input image
        self.img_height, self.img_width = self.img.shape[:2]

        # Convert the image color space from BGR to RGB
        img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)

        # Resize the image to match the input shape
        img = cv2.resize(img, (self.input_width, self.input_height))

        # Normalize the image data by dividing it by 255.0
        image_data = np.array(img) / 255.0

        # Transpose the image to have the channel dimension as the first dimension
        image_data = np.transpose(image_data, (2, 0, 1))  # Channel first

        # Expand the dimensions of the image data to match the expected input shape
        image_data = np.expand_dims(image_data, axis=0).astype(np.float32)

        # Return the preprocessed image data
        return image_data

    def postprocess(self, input_image, output):
        """
        Performs post-processing on the model's output to extract bounding boxes, scores, and class IDs.

        Args:
            input_image (numpy.ndarray): The input image.
            output (numpy.ndarray): The output of the model.

        Returns:
            numpy.ndarray: The input image with detections drawn on it.
        """
        
        # Transpose and squeeze the output to match the expected shape
        outputs = np.transpose(np.squeeze(output[0]))
        
        # Get the number of rows in the outputs array
        rows = outputs.shape[0]

        # Lists to store the bounding boxes, scores, and class IDs of the detections
        boxes = []
        scores = []
        class_ids = []

        # Calculate the scaling factors for the bounding box coordinates
        x_factor = self.img_width / self.input_width
        y_factor = self.img_height / self.input_height

        # Iterate over each row in the outputs array
        for i in range(rows):
            # Extract the class scores from the current row
            classes_scores = outputs[i][4:]

            # Find the maximum score among the class scores
            max_score = np.amax(classes_scores)

            # If the maximum score is above the confidence threshold
            if max_score >= self.confidence_thres:
                # Get the class ID with the highest score
                class_id = np.argmax(classes_scores)

                # Extract the bounding box coordinates from the current row
                x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]

                # Calculate the scaled coordinates of the bounding box
                left = int((x - w / 2) * x_factor)
                top = int((y - h / 2) * y_factor)
                width = int(w * x_factor)
                height = int(h * y_factor)

                # Add the class ID, score, and box coordinates to the respective lists
                class_ids.append(class_id)
                scores.append(max_score)
                boxes.append([left, top, width, height])

        # Apply non-maximum suppression to filter out overlapping bounding boxes
        indices = cv2.dnn.NMSBoxes(boxes, scores, self.confidence_thres, self.iou_thres)

        # Iterate over the selected indices after non-maximum suppression
        for i in indices:
            # Get the box, score, and class ID corresponding to the index
            box = boxes[i]
            score = scores[i]
            class_id = class_ids[i]

            # Draw the detection on the input image
            self.draw_detections(input_image, box, score, class_id)

        # Return the modified input image
        return input_image

    def main(self):
        """
        Performs inference using an ONNX model and returns the output image with drawn detections.

        Returns:
            output_img: The output image with drawn detections.
        """
        # Create an inference session using the ONNX model and specify execution providers
        session = ort.InferenceSession(self.onnx_model, providers=["CUDAExecutionProvider", "CPUExecutionProvider"])

        # Get the model inputs
        model_inputs = session.get_inputs()

        # Store the shape of the input for later use
        input_shape = model_inputs[0].shape
        self.input_width = input_shape[2]
        self.input_height = input_shape[3]

        # Preprocess the image data
        img_data = self.preprocess()

        # Run inference using the preprocessed image data
        outputs = session.run(None, {model_inputs[0].name: img_data})

        # Perform post-processing on the outputs to obtain output image.
        return self.postprocess(self.img, outputs)  # output image


if __name__ == "__main__":
    # Create an argument parser to handle command-line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="yolov8n.onnx", help="Input your ONNX model.")
    parser.add_argument("--img", type=str, default=str(ASSETS / "bus.jpg"), help="Path to input image.")
    parser.add_argument("--conf-thres", type=float, default=0.5, help="Confidence threshold")
    parser.add_argument("--iou-thres", type=float, default=0.5, help="NMS IoU threshold")
    args = parser.parse_args()

    # Check the requirements and select the appropriate backend (CPU or GPU)
    check_requirements("onnxruntime-gpu" if torch.cuda.is_available() else "onnxruntime")

    # Create an instance of the YOLOv8 class with the specified arguments
    detection = YOLOv8(args.model, args.img, args.conf_thres, args.iou_thres)

    # Perform object detection and obtain the output image
    output_image = detection.main()

    # Display the output image in a window
    cv2.namedWindow("Output", cv2.WINDOW_NORMAL)
    cv2.imshow("Output", output_image)
    cv2.imwrite("test_onnx_res.jpg", output_image)
    # Wait for a key press to exit
    cv2.waitKey(10000)
    cv2.destroyAllWindows()
```

# é™„å½•3: PCä¸Šæ¨¡æ‹Ÿè¿è¡Œç›®æ ‡æ£€æµ‹
```python
import os
import cv2
import sys
import argparse

from rknn.api import RKNN

import numpy as np

OBJ_THRESH = 0.25
NMS_THRESH = 0.45

IMG_SIZE = (1920, 1920)  # (width, height), such as (1280, 736)

CLASSES = ("person", "bicycle", "car","motorbike ","aeroplane ","bus ","train","truck ","boat","traffic light",
           "fire hydrant","stop sign ","parking meter","bench","bird","cat","dog ","horse ","sheep","cow","elephant",
           "bear","zebra ","giraffe","backpack","umbrella","handbag","tie","suitcase","frisbee","skis","snowboard","sports ball","kite",
           "baseball bat","baseball glove","skateboard","surfboard","tennis racket","bottle","wine glass","cup","fork","knife ",
           "spoon","bowl","banana","apple","sandwich","orange","broccoli","carrot","hot dog","pizza ","donut","cake","chair","sofa",
           "pottedplant","bed","diningtable","toilet ","tvmonitor","laptop	","mouse	","remote ","keyboard ","cell phone","microwave ",
           "oven ","toaster","sink","refrigerator ","book","clock","vase","scissors ","teddy bear ","hair drier", "toothbrush ")



def xywh2xyxy(*box):
    ret = [box[0] - box[2] // 2, box[1] - box[3] // 2, box[0] + box[2] // 2, box[1] + box[3] // 2]
    return ret

def get_inter(box1, box2):
    x1, y1, x2, y2 = xywh2xyxy(*box1)
    x3, y3, x4, y4 = xywh2xyxy(*box2)
    if x1 >= x4 or x2 <= x3:
        return 0
    if y1 >= y4 or y2 <= y3:
        return 0
    x_list = sorted([x1, x2, x3, x4])
    x_inter = x_list[2] - x_list[1]
    y_list = sorted([y1, y2, y3, y4])
    y_inter = y_list[2] - y_list[1]
    inter = x_inter * y_inter
    return inter

def get_iou(box1, box2):
    box1_area = box1[2] * box1[3]
    box2_area = box2[2] * box2[3]
    inter_area = get_inter(box1, box2)
    union = box1_area + box2_area - inter_area
    iou = inter_area / union
    return iou

def nms(pred, conf_thres, iou_thres):
    box = pred[pred[..., 4] > conf_thres]
    cls_conf = box[..., 5:]
    cls = []
    for i in range(len(cls_conf)):
        cls.append(int(np.argmax(cls_conf[i])))
    total_cls = list(set(cls))
    output_box = []
    output_class = []
    output_conf = []
    for i in range(len(total_cls)):
        clss = total_cls[i]
        cls_box = []
        temp = box[:, :6]
        for j in range(len(cls)):
            if cls[j] == clss:
                temp[j][5] = clss
                cls_box.append(temp[j][:6])
        cls_box = np.array(cls_box)
        sort_cls_box = sorted(cls_box, key = lambda x : -x[4])

        max_conf_box = sort_cls_box[0]
        output_box.append(max_conf_box)
        output_class.append(int(max_conf_box[4]))
        output_conf.append(max_conf_box[5])
        sort_cls_box = np.delete(sort_cls_box, 0, 0)

        while len(sort_cls_box) > 0:
            max_conf_box = output_box[-1]
            del_index = []
            for j in range(len(sort_cls_box)):
                current_box = sort_cls_box[j]
                iou = get_iou(max_conf_box, current_box)
                if iou > iou_thres:
                    del_index.append(j)
            sort_cls_box = np.delete(sort_cls_box, del_index, 0)
            if len(sort_cls_box) > 0:
                output_box.append(sort_cls_box[0])
                output_class.append(int(sort_cls_box[0][4]))
                output_conf.append(sort_cls_box[0][5])
                sort_cls_box = np.delete(sort_cls_box, 0, 0)
    return output_box, output_class, output_conf

def cod_trf(result, pre, after):
    res = np.array(result)
    x, y, w, h, conf, cls = res.transpose((1, 0))
    x1, y1, x2, y2 = xywh2xyxy(x, y, w, h)
    h_pre, w_pre, _ = pre.shape
    h_after, w_after, _ = after.shape
    scale = max(w_pre / w_after, h_pre / h_after)
    h_pre, w_pre = h_pre / scale, w_pre / scale
    x_move, y_move = abs(w_pre - w_after) // 2, abs(h_pre - h_after) // 2
    ret_x1, ret_x2 = (x1 - x_move) * scale, (x2 - x_move) * scale
    ret_y1, ret_y2 = (y1 - y_move) * scale, (y2 - y_move) * scale
    ret = np.array([ret_x1, ret_y1, ret_x2, ret_y2, conf, cls]).transpose((1, 0))
    return ret

def post_process(output):
    # (1, 13, 75600) -> box4[xywh], conf1, cls1, xxx
    pred = np.squeeze(output)
    pred = np.transpose(pred, (1, 0))
    pred_class = pred[..., 4:]
    pred_conf = np.max(pred_class, axis = -1)
    pred = np.insert(pred, 4, pred_conf, axis = -1)

    box, classes, confs = nms(pred, 0.5, 0.5)
    return box, classes, confs

def draw(image, boxes, scores, classes):
    for box, score, cl in zip(boxes, scores, classes):
        top, left, right, bottom = [int(_b) for _b in box]
        print("%s @ (%d %d %d %d) %.3f" % (CLASSES[cl], top, left, right, bottom, score))
        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)
        cv2.putText(image, '{0} {1:.2f}'.format(CLASSES[cl], score),
                    (top, left - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

def convert(model_path = '../model/v8n.onnx'):
    rknn = RKNN(verbose = False, verbose_file = 'build.log')
    rknn.config(mean_values=[128, 128, 128], std_values=[128, 128, 128], quant_img_RGB2BGR = True, quantized_method = 'channel', target_platform = 'rk3588')
    export_outputs = ['/model.22/Mul_2_output_0', '/model.22/Split_output_1']
    ret = rknn.load_onnx(model = model_path, outputs = export_outputs)
    #ret = rknn.load_pytorch(model = '../model/v8n.pt', input_size_list = [[1, 3, 1920, 1920]])
    ret = rknn.build(do_quantization = False)
    ret = rknn.export_rknn(export_path = 'v8n.rknn', simplify = False)
    return rknn

if __name__ == '__main__':
    rknn = convert()
    
    imglist = ['../model/test.jpg']
    if False:
        path = '/home/channy/Documents/thirdlibs/rknn_model_zoo/datasets/COCO/subset'
        for root, ds, fs in os.walk(path):
            for f in fs:
                imglist.append(root + '/' + f)
        ret = rknn.accuracy_analysis(inputs = ['../model/test.jpg'], target = None)
    
    origin_img = cv2.imread('../model/test.jpg')
    origin_img_rz = cv2.resize(origin_img, IMG_SIZE)
    img_height, img_width = origin_img.shape[:2]
    img = cv2.cvtColor(origin_img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, IMG_SIZE)
    image_data = np.array(img) / 255.0
    image_data = np.transpose(image_data, (0, 1, 2))
    image_data = np.expand_dims(image_data, axis = 0).astype(np.float16)
    
    ret = rknn.load_rknn(path = 'v8n.rknn')
    ret = rknn.init_runtime(target = None, eval_mem = False, perf_debug = False)
    print(rknn.get_sdk_version())
    outputs = rknn.inference(inputs = [img])#, data_format = 'nchw')  
    output = np.concatenate((outputs[0], outputs[1]), axis = 1)
    boxes, classes, scores = post_process(output)
    xyxyboxes = []
    for b in boxes:
        xyxyboxes.append(xywh2xyxy(b[0], b[1], b[2], b[3]))
    draw(origin_img_rz, xyxyboxes, scores, classes)
    cv2.imshow('res', cv2.resize(origin_img_rz, (750, 750)))
    cv2.imwrite('test_res.jpg', cv2.resize(origin_img_rz, (750, 750)))
    cv2.waitKey(10000)
    cv2.destroyAllWindows()
    # only support in device, not in simulator
    #ret = rknn.eval_perf()
    #ret = rknn.eval_memory()
    rknn.release()
```
