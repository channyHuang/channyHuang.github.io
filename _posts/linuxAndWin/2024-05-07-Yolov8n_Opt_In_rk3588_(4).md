---
layout: default
title: Yolov8n Opt In rk3588 (4)
categories:
- Linux
tags:
- Server
- Linux
---
//Description: 记录yolov8n部署到rk3588的后续修改及优化，如分别尝试多线程、协程、多进程并进行帧率对比、对超大图像进行分割分别识别再合并、对多个模型进行串联或并联识别等。

//Create Date: 2024-05-07 09:14:28

//Author: channy

[toc]

# 背景问题
## 问题1：超大图像识别不实时
经过前面的尝试，知道了模型的输入尺寸1920和640对帧率有明显的影响。考虑到超大尺寸的图像直接识别达不到实时需求，故对图像进行分割。

## 问题2：单模型识别大小目标效果不理想
当需要识别的目标种类多且大小相差较大时，使用单个模型识别达不到优秀的效果。

# 解决方案
## 超大图像分割
以尺寸1920X1080的输入图像为例，使用的模型输入尺寸为640（即pt转onnx的转换参数imgsz=640），初始时尝试直接对图像平均分割成4等分，则每份为960X540，再resize到640后进行识别，发现识别帧率只有17+。分析npu3个核占用率分别为38%、18%、18%，时间花费在同步等待上。后改成分割成2X3=6份，每份都为640X640，有重叠。识别帧率提升到20+。

源码片段见附录。

## 多模型串并联
使用两个模型，其中一个模型识别大目标，另一个识别小目标。考虑到多线程开的3个线程占满了3个核，故先考虑模型串联。尝试发现串联前后帧率相近，均为20+，npu占用率每个核都从18%上升到38%。

# 附录1：图像分割成640倍数计算offset
```python
import math

MODEL_SIZE = 640

def cal_offset(shape = [1080, 1920]):
    height_num = math.ceil(shape[0] / MODEL_SIZE)
    width_num = math.ceil(shape[1] / MODEL_SIZE)
    block = height_num * width_num
    height_pad, width_pad = 0, 0
    if height_num > 1:
        height_pad = (height_num * MODEL_SIZE - shape[0]) // (height_num - 1)
    if width_num > 1:
        width_pad = (width_num * MODEL_SIZE - shape[1]) // (width_num - 1)
    height_offset = 0
    width_offset = 0
    offset = []
    edge_flag = [False, False]
    while height_offset < shape[0]:
        edge_flag[1] = False
        if height_offset + MODEL_SIZE >= shape[0]:
            height_offset = shape[0] - MODEL_SIZE
            edge_flag[0] = True
        while width_offset < shape[1]:
            if width_offset + MODEL_SIZE > shape[1]:
                width_offset = shape[1] - MODEL_SIZE
                edge_flag[1] = True
            offset.append([height_offset, width_offset])
            if edge_flag[1]:
                break
            width_offset += MODEL_SIZE - width_pad
        if edge_flag[0]:
            break
        height_offset += MODEL_SIZE - height_pad
        width_offset = 0
    return offset
```

# 附录2：图像分片输入线程池源码片段
```python
        origin_img, _, _ = stream_queue.get()
        shape = origin_img.shape
        for i in range(block):
            img = origin_img[offset[i][0]:offset_end[i][0], offset[i][1]:offset_end[i][1]]
            source_img = copy.deepcopy(img)
            pool.put(source_img)

        flag = True
        final_img = origin_img
        for i in range(block):
            data_tuple, flag = pool.get()
            if not flag:
                print('flag false')
                break
            final_clip, classes = data_tuple[0], data_tuple[1]
            final_img[offset[i][0]:offset_end[i][0], offset[i][1]:offset_end[i][1]] = final_clip
            frames += 1
            if frames % 30 == 0:
                spend_time = "30 frame average fps: {:.2f}".format(round(30 / (time.time() - loopTime), 2))
                loopTime = time.time()
        
        cv2.putText(final_img, spend_time, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.imshow('res', final_img)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
```

# 附录3：协程
```python
import asyncio
import cv2
import argparse
import time

from rknnpool import *
from postprocess import *

def init(args):
    co_helper = Simple_COCO_test_helper(enable_letter_box=True)
    rknns = [initRKNN(args.model, id) for id in range(3)]
    return rknns, co_helper

async def image_inference(rknn, co_helper, origin_img):
    start_time = time.time()
    origin_img = cv2.resize(origin_img, IMG_SIZE) 
    img, _ = detect_frame(rknn, origin_img, co_helper)
    final_img = cv2.resize(img, (750, 750))
    spend_time = "per frame time : {:.2f} s".format(time.time() - start_time)
    cv2.putText(final_img, spend_time, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
    cv2.imshow('res', final_img)
    cv2.waitKey(1)

async def inference_frame(origin_img, id, rknns, co_helper):
    await image_inference(rknns[id], co_helper, origin_img)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='--model model_path --uri video_path')
    parser.add_argument('--model', type = str, default = '../model/yolov8n.rknn', help = '.rknn')
    parser.add_argument('--uri', type=str, default='../model/test.mp4', help='uri')
    args = parser.parse_args()

    rknns, co_helpter = init(args)
    loop = asyncio.get_event_loop()
    cap = cv2.VideoCapture(args.uri)
    frames = 0
    while cap.isOpened():
        ret, org_img = cap.read()
        if not ret:
            break
        frames += 1
        tasks = [asyncio.ensure_future(inference_frame(org_img, frames % 3, rknns, co_helpter))]
        loop.run_until_complete(asyncio.wait(tasks))
```
1