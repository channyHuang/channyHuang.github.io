---
layout: default
title: Yolov8n Opt In rk3588 (2)
categories:
- Linux
tags:
- Server
- Linux
---
//Description: 记录yolov8n部署到rk3588后的优化，从原先的单核npu跑修改成使用线程池开3个线程3个核一起跑，本地视频的识别帧率从原来的14+提升到40+，但网络摄像头的识别帧率依旧在15+，最终发现和摄像头支持的最高帧率及当前设置的帧率有关系。

//Create Date: 2024-04-20 10:14:28

//Author: channy

[toc]

# 使用线程池进行多frame检测
## 背景
使用自己训练的模型：
* **方法**： 原先使用单线程，直接从摄像头中读取一帧并进行识别，识别完成后显示，再读取下一帧
* **时间性能**： 本地视频识别帧率14+，其中python推理接口0.05s/帧，后处理0.02s/帧
* **其它性能**： 内存100+M，cpu20%左右，npu核1达20%左右，核2和核3基本没用上，为0% 
* **优化**： 使用线程池开启3个线程，把3个核跑满
* **优化后时间性能**： 本地视频识别帧率40+，网络摄像头识别帧率15+。后发现网络摄像头设置的帧率为15。
* **优化后其它性能**： 识别本地视频时，内存100+M，cpu50%左右，npu每个核都占40%左右  

源代码见附录。

# C++输入不同尺寸图像报错
模型输入尺寸是640x640的。
## 1. 报`src unsupport width stride 750`错误
输入图像尺寸是750x750时报错，原因为rga只支持16位对齐的数据，把图像先resize为736x736的，该错误消失。

也就是说，rknn_model_zoo里面是没有对输入图像尺寸非16的倍数这种情况做处理的。
```sh
$ ./rknn_yolov8_demo ../model/v8n_640.rknn ../model/test.jpg

Error on improcess STATUS=-1
RGA error message: Unsupported function: src unsupport width stride 750, rgb888 width stride should be 16 aligned!

// convert_image_rga in image_utils.c 
```

修改方法可以在convert_image_rga函数中improcess之前增加非16倍数的图像做裁剪或填充处理，类似长宽尺寸不相等时的填充处理部分。具体代码稍后补充。

## 2. rga输出一堆类似错误的信息
输入图像尺寸是2880x1616时不算报错，但在首帧识别结束运行第二帧的时候，会停止在improcess之前不往下走，有点像死锁的现象。
```sh
$ ./rknn_yolov8_demo ../model/v8n_640.rknn ./out.jpg

fill dst image (x y w h)=(0 0 640 640) with color=0x72727272
 RgaCollorFill(1819) RGA_COLORFILL fail: Invalid argument
 RgaCollorFill(1820) RGA_COLORFILL fail: Invalid argument
69 im2d_rga_impl rga_task_submit(2171): Failed to call RockChipRga interface, please use 'dmesg' command to view driver error log.
69 im2d_rga_impl rga_dump_channel_info(1500): src_channel: 
  rect[x,y,w,h] = [0, 0, 0, 0]
  image[w,h,ws,hs,f] = [0, 0, 0, 0, rgba8888]
  buffer[handle,fd,va,pa] = [0, 0, 0, 0]
  color_space = 0x0, global_alpha = 0x0, rd_mode = 0x0

69 im2d_rga_impl rga_dump_channel_info(1500): dst_channel: 
  rect[x,y,w,h] = [0, 0, 640, 640]
  image[w,h,ws,hs,f] = [640, 640, 640, 640, rgb888]
  buffer[handle,fd,va,pa] = [154, 0, 0, 0]
  color_space = 0x0, global_alpha = 0xff, rd_mode = 0x1

69 im2d_rga_impl rga_dump_opt(1550): opt version[0x0]:

69 im2d_rga_impl rga_dump_opt(1551): set_core[0x0], priority[0]

69 im2d_rga_impl rga_dump_opt(1554): color[0x72727272] 
69 im2d_rga_impl rga_dump_opt(1563): 

69 im2d_rga_impl rga_task_submit(2180): acquir_fence[-1], release_fence_ptr[0x0], usage[0x280000]

// 使用线程读取摄像头的第二帧时，停在此处
```

修改方法待补充

# 附录：源代码（部分）

```python
# rknnpool.py
from concurrent.futures import ThreadPoolExecutor, as_completed
import cv2
import numpy as np
from queue import Queue
from rknnlite.api import RKNNLite

# copy from coco_utils.py in rknn_model_zoo
class Simple_Letter_Box_Info():
    def __init__(self, shape, new_shape, w_ratio, h_ratio, dw, dh, pad_color) -> None:
        self.origin_shape = shape
        self.new_shape = new_shape
        self.w_ratio = w_ratio
        self.h_ratio = h_ratio
        self.dw = dw 
        self.dh = dh
        self.pad_color = pad_color

class Simple_COCO_test_helper():
    def __init__(self, enable_letter_box = False) -> None:
        self.record_list = []
        self.enable_ltter_box = enable_letter_box
        if self.enable_ltter_box is True:
            self.letter_box_info_list = []
        else:
            self.letter_box_info_list = None

    def letter_box(self, im, new_shape, pad_color=(0,0,0), info_need=False):
        # Resize and pad image while meeting stride-multiple constraints
        shape = im.shape[:2]  # current shape [height, width]
        if isinstance(new_shape, int):
            new_shape = (new_shape, new_shape)

        # Scale ratio
        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])

        # Compute padding
        ratio = r  # width, height ratios
        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding

        dw /= 2  # divide padding into 2 sides
        dh /= 2

        if shape[::-1] != new_unpad:  # resize
            im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)
        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
        im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=pad_color)  # add border
        
        if self.enable_ltter_box is True:
            self.letter_box_info_list.append(Simple_Letter_Box_Info(shape, new_shape, ratio, ratio, dw, dh, pad_color))
        if info_need is True:
            return im, ratio, (dw, dh)
        else:
            return im
    
    def get_real_box(self, box, in_format='xyxy'):
        bbox = box
        if self.enable_ltter_box == True:
        # unletter_box result
            if in_format=='xyxy':
                bbox[:,0] -= self.letter_box_info_list[-1].dw
                bbox[:,0] /= self.letter_box_info_list[-1].w_ratio
                bbox[:,0] = np.clip(bbox[:,0], 0, self.letter_box_info_list[-1].origin_shape[1])

                bbox[:,1] -= self.letter_box_info_list[-1].dh
                bbox[:,1] /= self.letter_box_info_list[-1].h_ratio
                bbox[:,1] = np.clip(bbox[:,1], 0, self.letter_box_info_list[-1].origin_shape[0])

                bbox[:,2] -= self.letter_box_info_list[-1].dw
                bbox[:,2] /= self.letter_box_info_list[-1].w_ratio
                bbox[:,2] = np.clip(bbox[:,2], 0, self.letter_box_info_list[-1].origin_shape[1])

                bbox[:,3] -= self.letter_box_info_list[-1].dh
                bbox[:,3] /= self.letter_box_info_list[-1].h_ratio
                bbox[:,3] = np.clip(bbox[:,3], 0, self.letter_box_info_list[-1].origin_shape[0])
        return bbox

# rknn pool
def initRKNN(model = '', id = -1):
    rknn = RKNNLite()
    ret = rknn.load_rknn(model)
    if ret != 0:
        print('load rknn model failed:', model)
        exit(ret)
    if id == 0:
        ret = rknn.init_runtime(core_mask = RKNNLite.NPU_CORE_0)
    elif id == 1:
        ret = rknn.init_runtime(core_mask = RKNNLite.NPU_CORE_1)
    elif id == 2:
        ret = rknn.init_runtime(core_mask = RKNNLite.NPU_CORE_2)
    else:
        ret = rknn.init_runtime(core_mask = RKNNLite.NPU_CORE_0_1_2)
    if ret != 0:
        print('Init runtime failed')
        exit(ret)
    return rknn

def initRKNNs(model_path, num_model = 1):
    rknn_list = []
    for i in range(num_model):
        rknn_list.append(initRKNN(model_path, i % 3))
    return rknn_list 

class rknnPoolExecutor():
    def __init__(self, model_path, num_model, func):
        self.num_model = num_model
        self.queue = Queue()
        self.rknnPool = initRKNNs(model_path, num_model)
        self.pool = ThreadPoolExecutor(max_workers = num_model)
        self.func = func
        self.num = 0
        self.co_helper = Simple_COCO_test_helper(enable_letter_box=True)

    def put(self, frame):
        self.queue.put(self.pool.submit(self.func, self.rknnPool[self.num % self.num_model], frame, self.co_helper))
        self.num += 1
    
    def get(self):
        if self.queue.empty():
            return None
        temp = []
        temp.append(self.queue.get())
        for frame in as_completed(temp):
            return frame.result(), True
    
    def release(self):
        self.pool.shutdown()
        for rknn_lite in self.rknnPool:
            rknn_lite.release()
```

```python
# main (part)
# detect_frame: input frame, output detect result image
def video_inference_new(args):
    num_model = 3
    pool = rknnPoolExecutor('./yolov8n.rknn', num_model, detect_frame)

    print('done')
    cap = cv2.VideoCapture(args.uri)
    spend_time = ""

    if (cap.isOpened()):
        for i in range(num_model):
            ret, org_img = cap.read()
            if not ret:
                exit(-1)
            origin_img = copy.deepcopy(org_img)
            origin_img = cv2.resize(origin_img, IMG_SIZE) 
            pool.put(origin_img)

    frames, loopTime = 0, time.time()
    while (cap.isOpened()):
        frames += 1
        ret, org_img = cap.read()
        if not ret:
            break
        origin_img = copy.deepcopy(org_img)
        origin_img = cv2.resize(origin_img, IMG_SIZE) 
        pool.put(origin_img)
        final_img, flag = pool.get()
        
        if frames % 30 == 0:
            spend_time = "30 frame average fps: {:.2f}".format(round(30 / (time.time() - loopTime), 2))
            loopTime = time.time()
        
        cv2.putText(final_img, spend_time, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.imshow('res', final_img)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cap.release()
    cv2.destroyAllWindows()
    pool.release()
```
