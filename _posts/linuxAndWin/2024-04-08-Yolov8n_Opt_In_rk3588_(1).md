---
layout: default
title: Yolov8n Opt In rk3588 (1)
categories:
- Linux
tags:
- Server
- Linux
---
//Description: 记录yolov8n部署到rk3588后的修改和优化，包括剪枝、量化、混合量化、转成c++，然而并没有明显优化效果，转成c++后每帧从python的0.5s降到0.35s，主要耗时均在SDK的三个api中(rknn_init,rknn_inputs_set,rknn_run)，每个api用时0.1+s，整个识别过程就特别慢。最终通过模型不同的输出测试，确认为输入分辨率对识别耗时有明显影响，同时pt转onnx的方式有问题，应该使用SDK官网的转换方式并设置imgsz而不是使用Yolov8官网的转换方式。写于2024年4月初，SDK文档参考了v2.0.0beta0版本，板子实际运行安装的SDK是1.6.0版本。

//Create Date: 2024-04-08 10:14:28

//Author: channy

[toc]

# 剪枝
`model_pruning`设置为True剪枝，然而对小模型没有明显效果。
```python
    rknn.config(mean_values=[128, 128, 128], std_values=[128, 128, 128], 
                quant_img_RGB2BGR = True, 
                quantized_dtype = 'asymmetric_quantized-8', quantized_method = 'layer', quantized_algorithm = 'mmse', optimization_level = 3,
                target_platform = 'rk3588',
                model_pruning = True)
```

# 量化
量化方法channel/layer，量化算法normal/mmse/kl_divergence
```python
    ret = rknn.build(do_quantization = True, dataset = './dataset.txt', rknn_batch_size = None)
```
当`quant_img_RGB2BGR`为False时，error_analysis的误差上升到几百+，改成True后依旧几百上千+。
## 混合量化
```python
    # step 1
    if True:
        model_path = '../model/v8n.onnx'
        rknn = RKNN(verbose = True)
        rknn.config(mean_values=[128, 128, 128], std_values=[128, 128, 128], 
                    quant_img_RGB2BGR = True, 
                    quantized_dtype = 'asymmetric_quantized-8', quantized_method = 'channel', quantized_algorithm = 'normal', optimization_level = 3,
                    target_platform = 'rk3588',
                    model_pruning = True)
        ret = rknn.load_onnx(model = model_path)
        ret = rknn.hybrid_quantization_step1(dataset = './dataset.txt', proposal = False)
        rknn.release()
    # step 2
    rknn = RKNN(verbose = True)
    ret = rknn.hybrid_quantization_step2(model_input = './v8n.model',
                                         data_input='./v8n.data',
                                         model_quantization_cfg='./v8n.quantization.cfg')
    ret = rknn.export_rknn('./v8n.rknn')
    ret = rknn.accuracy_analysis(inputs = ['../model/test.jpg'], output_dir = None)
    start_time = time.time()
    origin_img = cv2.imread('../model/test.jpg')
    origin_img_rz = cv2.resize(origin_img, IMG_SIZE)
    img_height, img_width = origin_img.shape[:2]
    img = cv2.cvtColor(origin_img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, IMG_SIZE)
    image_data = np.array(img) / 255.0
    image_data = np.transpose(image_data, (0, 1, 2))
    image_data = np.expand_dims(image_data, axis = 0).astype(np.float16)
    
    ret = rknn.init_runtime()
    outputs = rknn.inference(inputs = [img])

    boxes, classes, scores = post_process(outputs)
    xyxyboxes = []
    for b in boxes:
        xyxyboxes.append(xywh2xyxy(b[0], b[1], b[2], b[3]))
    draw(origin_img_rz, xyxyboxes, scores, classes)

    cur_time = time.time()
    spend_time = "{:.2f}  s".format((cur_time - start_time))
    cv2.putText(origin_img_rz, spend_time, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
    cv2.imshow('res', cv2.resize(origin_img_rz, (750, 750)))
    #cv2.imwrite('test_res.jpg', cv2.resize(origin_img_rz, (750, 750)))
    cv2.waitKey(10000)
    cv2.destroyAllWindows()
```
量化完后发现使用自己的模型完全识别不到目标。。。

# python转c++
背景：使用python写读入图像、推理、画框显示整个过程总耗时在0.5s/帧左右，耗时集中在python的rknn推理接口。

参考yolov8的demo中c++的实现，主要文件有3个：
* main.cc
* yolov8.cc
* postprocess.cc
主要函数有：
```c++
init_post_process //从文件中读取label
	loadLabelName
init_yolov8_model //加载模型，调用rknn_init和rknn_query两个api接口
read_image //读取图像
inference_yolov8_model //推理，调用rknn_run
	convert_image_with_letterbox 
		convert_image 
			convert_image_rga
	rknn_inputs_set
	rknn_run
	rknn_outputs_get
	post_process //后处理，根据threshold过滤检测结果
        process_fp32 // threshold过滤
write_image //写结果图像到文件
```

自己的模型改成c++的话主要修改post_process的后处理函数。

见附录改成自己的模型后，对单图像耗时进行测试，发现如下：
1. 图像输入尺寸影响耗时
过程中输出错误
```
Error on improcess STATUS=-1
RGA error message: Unsupported function: src unsupport with stride 750, rgb888 width stride should be 16 aligned!
```
模型是1920x1920，输入图像使用了750x750，报了上面的错误，改成736x736后错误消失，总运行时间上也从0.47s降到0.37s。

2. 不同模型识别耗时不一样，集中在SDK的api中
* Python的耗时集中在rknn.inference上，自己的模型用时0.47+s/帧
* c++的耗时集中在rknn_init、rknn_inputs_set和rknn_run三个函数中，自己的模型用时分别为0.09+s/帧、0.11+s/帧、0.13+s/帧，总用时0.35+s/帧。

3. 图像输入使用的是RawRGB原始数据，即width*height*channel个char的数据可直接输入，jpg的数据需要使用SDK里面的imgutils库转化。

# 模型耗时和demo相差大
做了以上优化后，发现自己的模型耗时和demo的差距依旧较大，故查找原因。先后试过不同的模型输入分辨率并记录Python单推理接口耗时/帧如下。 
 
| 模型输入分辨率 | 模型训练代码版本 | 激活函数 | onnx转换方式 | SDK转换参数 | Python单推理接口耗时/帧 | 本地视频全流程帧率(fps) |
|:---:|:------:|:---:|:---:|:---:|:---:|:---:|
| 1920 x 1920 | Yolov5n | Relu | Yolov8官网转换 | - | 0.25s |
| 1920 x 1920 |	Yolov8n | Softmax | Yolov8官网转换 | - | 0.5s |
| 1920 x 1920 | Yolov8n | Softmax | SDK官网转换 | imgsz=640 | <font color=red>0.06s</font> |
| 1920 x 1920 | Yolov8n | Softmax | SDK官网转换 | imgsz = 1920 | 0.42s |
| 1920 x 1920 |	Yolov8n | Relu | Yolov8官网转换 | - | 0.5s |
| 1920 x 1920 |	Yolov8n | Relu | SDK官网转换 | imgsz=640 | <font color=red>0.06s</font> |
| 1920 x 1920 |	Yolov8n | Relu | SDK官网转换 | imgsz=1920 | <font color=red>0.06s</font> | 20 
| 640 x 640 | Yolov8n | Relu | SDK官网转换 | imgsz=640 | <font color=red>0.04s</font> | 40
| 640 x 640 | Yolov8n | Relu | SDK官网转换 | imgsz=1920 | 0.42s |

最终确认耗时大的主要原因是分辨率大。同时注意到`failed to config argb mode layer`错误。

## ONNX转成RKNN报'failed to config argb mode layer'错误
在试验过程中遇到在用Yolov8的`export`函数直接转成`onnx`后再转`RKNN`报`failed to config argb mode layer`错误。

```sh
I rknn building ...
E RKNN: [15:36:41.876] failed to config argb mode layer!
Aborted (core dumped)
```

```python
def pt2onnx(path = '../model/v8n_relu.pt', sim = True):
    model = YOLO(path)
    res = model.export(format="onnx", simplify = sim)#, opset = 19) 
```

上网搜索发现使用[ultralytics_yolov8](https://github.com/airockchip/ultralytics_yolov8.git)里面的`exporter.py`从`pt`转到`onnx`再转`rknn`能转成功。看描述应该是算子相关问题。

接着发现之前耗时大的真正原因是`pt`转`onnx`的转换方式也有问题，用Yolov8官网的转换方式是保持了输入尺寸即1920x1920的，分辨率大导致耗时大；用SDK官网的转换方式默认参数imgsz＝640，转换后才能达到和demo相当的水平；但其中用v8n+1920x1920+relu直接用imgsz=1920也能达到demo相当的速度。

# adb 连板分析
## adb connect [ip]
没有数据线，网线直连传输文件到板端，无意中发现adb除了使用数据线外，也可以使用网线直连的方式，只需要pc和板的ip在同一网段即可。
```
$ adb connect 192.168.1.222
connected to 192.168.1.222:5555
```
然后执行代码，可正常运行`memory_detail = rknn.eval_memory()`并输出内存占用状况。
```
======================================================
            Memory Profile Info Dump                  
======================================================
NPU model memory detail(bytes):
    Weight Memory: 8.06 MiB
    Internal Tensor Memory: 105.47 MiB
    Other Memory: 3.72 MiB
    Total Memory: 117.25 MiB

INFO: When evaluating memory usage, we need consider  
the size of model, current model size is: 11.79 MiB       
======================================================
```
## `rknn.eval_perf()`报错
在执行性能评估时发生
`invalid literal for int() with base 10: 'cat: /sys/class/devfreq/dmc/cur_freq: 没有那个文件或目录'`
的错误。
```
    perf_result = rknn.eval_perf()
  File "/home/channy/miniconda3/envs/RKNN-Toolkit2/lib/python3.8/site-packages/rknn/api/rknn.py", line 326, in eval_perf
    return self.rknn_base.eval_perf(is_print, fix_freq)
  File "rknn/api/rknn_base.py", line 3005, in rknn.api.rknn_base.RKNNBase.eval_perf
  File "rknn/api/rknn_runtime.py", line 223, in rknn.api.rknn_runtime.RKNNRuntime.get_hardware_freq
  File "rknn/api/rknn_platform.py", line 817, in rknn.api.rknn_platform.get_ddr_freq
ValueError: invalid literal for int() with base 10: 'cat: /sys/class/devfreq/dmc/cur_freq: 没有那个文件或目录'
```
板子`/sys/class/devfreq`下面确实没有`dmc`文件夹。即板子不支持固定DDR频率。而api的源码没有公布也改不了路径。。。

无解。。。只能自行使用其它方法统计总时间了。

# 附录：根据自己的模型修改后处理函数
```c++
static int process_fp32(float *box_tensor, int grid_w, int grid_h,
                        std::vector<float> &boxes, 
                        std::vector<float> &objProbs, 
                        std::vector<int> &classId, 
                        float threshold)
{
    int validCount = 0;
    int grid_len = grid_h;

    for (int i = 0; i < grid_h; i++)
    {
        int offset = i;
        int max_class_id = -1;

        float max_score = 0;
        int pad = (grid_len << 2);
        for (int c= 0; c< OBJ_CLASS_NUM; c++){
            if ((box_tensor[pad + offset] > threshold) && (box_tensor[pad + offset] > max_score))
            {
                max_score = box_tensor[pad + offset];
                max_class_id = c;
            }
            offset += grid_len;
        }

        // compute box
        if (max_score> threshold){
            offset = i;
            float box[4];
            for (int k=0; k < 4; k++){
                box[k] = box_tensor[offset];
                offset += grid_len;
            }

            float x1,y1,w,h;
            x1 = box[0];
            y1 = box[1];
            w = box[2];
            h = box[3];
            boxes.push_back(x1);
            boxes.push_back(y1);
            boxes.push_back(w);
            boxes.push_back(h);
            objProbs.push_back(max_score);
            classId.push_back(max_class_id);
            validCount ++;
        }
    }
    return validCount;
}
```
